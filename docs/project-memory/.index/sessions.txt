=== S-2026-02-18-0340-initial-build ===
# Session

Session-ID: S-2026-02-18-0340-initial-build
Title: Initial RAG service build — project scaffolding and full implementation
Date: 2026-02-18
Author: Claude + David

## Goal

Build the complete RAG service from the design doc: FastAPI server with document ingestion, vector search, drag-and-drop UI, and Docker packaging.

## Context

New standalone project for voice assistant RAG retrieval. Design doc approved at `docs/plans/2026-02-17-rag-service-design.md`. Traceable project memory system integrated from template repo.

## Plan

1. Set up project scaffolding (config, models, CLAUDE.md, project memory)
2. Implement core modules: embedder, vector store, parsers, chunker
3. Wire up document pipeline
4. Build FastAPI app with all routes
5. Create drag-and-drop web UI
6. Add Dockerfile and docker-compose.yml
7. Test end-to-end

## Changes Made

- Initialized git repo with project memory system
- Created design doc and implementation plan
- `config.py` — 11 env vars with defaults
- `models.py` — 10 Pydantic request/response schemas
- `chunker.py` — recursive character text splitter with tiktoken token counting
- `parsers.py` — file parsers for PDF (PyMuPDF), DOCX, Markdown, HTML (BeautifulSoup), TXT
- `embedder.py` — sentence-transformers wrapper with batch support (all-MiniLM-L6-v2, 384-dim)
- `vector_store.py` — LanceDB operations (insert, search, delete, list, stats) using PyArrow directly
- `document_pipeline.py` — orchestrator: parse → chunk → embed → store
- `app.py` — FastAPI app with 6 routes (/, /upload, /documents, /documents/{id}, /query, /health)
- `static/index.html` — drag-and-drop web UI, dark theme, document management, search test
- `Dockerfile` — Python 3.13-slim, pre-downloads model at build time
- `docker-compose.yml` — single-service setup with named volume
- `requirements.txt` — 11 dependencies pinned to actual latest versions
- 11 unit tests (5 chunker, 6 parsers) — all passing

## Decisions Made

- Used actual latest pip versions instead of spec's future versions. Pinned: fastapi==0.128.8, lancedb==0.27.1, sentence-transformers==5.1.2, etc.
- Used PyArrow directly in vector_store.py instead of pandas — avoids adding pandas as a dependency
- Score calculation uses `1/(1+distance)` instead of `1-distance` — more robust for non-normalized L2 distances
- Web UI uses safe DOM methods (textContent/createElement) instead of innerHTML for XSS prevention
- Used Python 3.13 venv for local dev (PEP 668 enforced on macOS)
- Integrated traceable-searchable-adr-memory-index for session/decision tracking

## Open Questions

- Query latency ~57ms warm (higher than spec's 20-30ms target) — could improve with cosine distance metric or index tuning
- Dockerfile not yet tested with `docker build` (tested local server only)

## Links

Commits:
- `9cb30db` Add config and Pydantic models
- `cbb1a27` Add recursive text chunker with tiktoken token counting
- `c4ee295` Add file parsers for PDF, DOCX, MD, HTML, TXT
- `077aaa3` Add embedding model wrapper with batch support
- `1702c97` Add LanceDB vector store with insert, search, delete, list
- `f64affd` Add document ingestion pipeline: parse, chunk, embed, store
- `2356b14` Add FastAPI app with all API routes
- `06ee7ea` Add drag-and-drop web UI for document management and search testing
- `fa8679c` Add Docker configuration

PRs:
- (none yet)

ADRs:
- (none yet — technology choices documented in design doc)

=== S-2026-02-18-0549-rag-tool-integration ===
# Session

Session-ID: S-2026-02-18-0549-rag-tool-integration
Title: Add RAG Tool to Voice Assistant
Date: 2026-02-18
Author: Claude

## Goal

Connect the RAG service (running on localhost:8100 with 690 indexed GitHub repo docs) to the voice assistant by adding a `RAGTool` that the LLM can call to answer questions about the user's projects.

## Context

- RAG service is live on port 8100 with documents from all of davidbmar's GitHub repos
- Voice assistant has a tool-calling framework with `BaseTool`, explicit registration in `__init__.py`, and `httpx` already in dependencies
- The `WebSearchTool` provides the pattern to follow

## Plan

1. Create `voice_assistant/tools/rag.py` — new `RAGTool` class following the `WebSearchTool` pattern
2. Modify `voice_assistant/tools/__init__.py` — import and register the new tool
3. Modify `voice_assistant/config.py` — add `rag_url` setting

## Changes Made

- Created `voice_assistant/tools/rag.py` with `RAGTool` class
  - POSTs to `/query` endpoint with `{"query": ..., "top_k": 5}`
  - Formats results as numbered list with repo name, relevance score, and text snippet
  - 2-second timeout, graceful failure if RAG service is down
  - URL configurable via `RAG_URL` env var (default: `http://localhost:8100`)
- Modified `voice_assistant/tools/__init__.py` — added import and `register_tool(RAGTool)`
- Modified `voice_assistant/config.py` — added `rag_url` field to both Settings variants

## Decisions Made

- Tool name `search_knowledge_base` chosen to distinguish from `web_search` — the LLM needs a clear signal about when to use each tool
- `top_k=5` as default — balances providing enough context without overwhelming the LLM's response
- 2-second timeout — RAG is local so should be fast; don't block the voice pipeline
- `document_id` used as repo identifier in output — the RAG service stores repo names as document IDs

## Open Questions

None.

## Links

Commits:
- (pending)

=== S-2026-02-18-0639-upgrade-embedding-model ===
# Session

Session-ID: S-2026-02-18-0639-upgrade-embedding-model
Title: Upgrade RAG embedding model to nomic-embed-text-v1.5
Date: 2026-02-18
Author: Claude

## Goal

Replace `all-MiniLM-L6-v2` (384-dim) with `nomic-ai/nomic-embed-text-v1.5` (768-dim) to improve retrieval quality, especially for code/technical content. Add task prefix support and auto-migration for dimension changes.

## Context

The current embedding model produces low relevance scores (0.43-0.46) and is weak on code/technical content. The nightly cron already does full re-indexing, so this is a clean swap. On startup, the vector store will detect the dimension mismatch and recreate the table.

## Plan

1. Update `config.py` — new default model + task prefix config
2. Update `embedder.py` — trust_remote_code + task prefixes
3. Update `vector_store.py` — dynamic schema + auto-migration
4. Update `Dockerfile` + `docker-compose.yml` — new model name
5. Update `README.md` — reflect new model

## Changes Made

- `config.py`: Default model → `nomic-ai/nomic-embed-text-v1.5`, added `TASK_PREFIX_MODELS` dict with substring-matched `(query_prefix, doc_prefix)` tuples
- `embedder.py`: Added `trust_remote_code=True` to constructor, `_get_task_prefixes()` helper, query prefix in `embed_text()`, doc prefix in `embed_batch()`
- `vector_store.py`: Replaced hard-coded 384-dim `SCHEMA` with `_build_schema(dim)`, added dimension mismatch detection in `init_store()` that drops and recreates the table
- `Dockerfile`: Updated model download and `ENV EMBEDDING_MODEL` to new model
- `docker-compose.yml`: Updated `EMBEDDING_MODEL` env var
- `README.md`: Updated model name, dimension (384→768), and technical decisions table
- `requirements.txt`: Added `einops>=0.8.0` — required by nomic's custom model code, not a transitive dep of sentence-transformers

## Decisions Made

- **Substring matching for task prefixes**: Used substring matching (`"nomic-embed-text"` in model name) rather than exact match so it handles org-prefix variations and version bumps
- **Auto-migration via drop+recreate**: On dimension mismatch, drop the existing table and create a new one. Safe because nightly cron does full re-index
- **trust_remote_code=True for all models**: Harmless for models that don't need it, required by nomic

## Open Questions

None — plan is well-defined.

## Links

Commits:
- (to be added)

PRs:
- (to be added)

ADRs:
- None needed — model swap is a straightforward upgrade

